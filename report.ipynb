{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrieval Pipeline Evaluation Report\n",
    "\n",
    "**Title:** *Evaluating Document Retrieval Quality using Fixed Token Chunking and MiniLM Embeddings on the State of the Union Corpus*  \n",
    "**Author:** Kyrylo Goroshenko  \n",
    "\n",
    "---\n",
    "\n",
    "## 1. Introduction\n",
    "\n",
    "This project focuses on building and evaluating a document retrieval pipeline using open-source tools. The retrieval system is tested on a real-world dataset—**State of the Union** addresses—by chunking the corpus, embedding both queries and chunks, and evaluating the system's ability to return relevant text.\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Dataset Preparation\n",
    "\n",
    "- **Corpus**: `state_of_the_union.md`\n",
    "- **Queries & References**: Extracted into `questions_state.csv` using a basic filtering script.\n",
    "- Each query in the CSV includes a `question` field and `references`, which are character-level (start, end) tuples marking relevant spans in the corpus.\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Chunking Strategy\n",
    "\n",
    "- **Algorithm**: `FixedTokenChunker`\n",
    "- **Source**: [GitHub - fixed_token_chunker.py](https://github.com/brandonstarxel/chunking_evaluation/blob/main/chunking_evaluation/chunking/fixed_token_chunker.py)\n",
    "- **Chunking Parameters**:\n",
    "  - Chunk sizes: 100, 200, 400, 600, 800\n",
    "  - Chunk overlaps: 0 to 400 (varied based on size)\n",
    "\n",
    "This helped analyze how different granularities and overlaps affect precision and recall.\n",
    "\n",
    "---\n",
    "\n",
    "## 4. Embedding Model\n",
    "\n",
    "- **Model**: `all-MiniLM-L6-v2`\n",
    "- **Source**: HuggingFace \n",
    "- **Embedding Function**: Implemented in `embedding_function.py` with batch support.\n",
    "\n",
    "---\n",
    "\n",
    "## 5. Evaluation Metrics\n",
    "\n",
    "- **Precision**: Proportion of retrieved chunk text overlapping with ground truth.\n",
    "- **Recall**: Proportion of ground truth captured in retrieved chunks.\n",
    "\n",
    "Implementation details:\n",
    "- Calculated based on range overlaps (using character-level spans)\n",
    "- Handles union of intersecting ranges\n",
    "\n",
    "---\n",
    "\n",
    "## 6. Retrieval Pipeline Overview\n",
    "\n",
    "The pipeline operates as follows:\n",
    "Corpus → Chunking → Chunk Embedding → Query Embedding → Similarity Search → Top-N Retrieval → Scoring\n",
    "\n",
    "- **Chunker**: `FixedTokenChunker`\n",
    "- **Retriever**: Cosine similarity over embeddings\n",
    "- **Evaluation**: Run for each query and averaged\n",
    "\n",
    "Implemented in `pipeline.ipynb`.\n",
    "\n",
    "---\n",
    "\n",
    "## 7. Experiments & Results\n",
    "\n",
    "- **Tested Retrieval Sizes**: 1, 2, 3, 5, 10\n",
    "- **Tested Chunk Sizes/Overlaps**: Several (see part of table below, or whole of it in `experiment_comparison_table.csv`)\n",
    "- All combinations were evaluated and logged."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>chunk_size</th>\n",
       "      <th>chunk_overlap</th>\n",
       "      <th>chunks_retieved</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>100</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.188042</td>\n",
       "      <td>0.486034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>100</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.120373</td>\n",
       "      <td>0.614665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>100</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0.093550</td>\n",
       "      <td>0.702523</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>100</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0.073808</td>\n",
       "      <td>0.897149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>100</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>0.039737</td>\n",
       "      <td>0.976826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>800</td>\n",
       "      <td>400</td>\n",
       "      <td>1</td>\n",
       "      <td>0.031116</td>\n",
       "      <td>0.608412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>800</td>\n",
       "      <td>400</td>\n",
       "      <td>2</td>\n",
       "      <td>0.021086</td>\n",
       "      <td>0.778466</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>800</td>\n",
       "      <td>400</td>\n",
       "      <td>3</td>\n",
       "      <td>0.014702</td>\n",
       "      <td>0.809186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>800</td>\n",
       "      <td>400</td>\n",
       "      <td>5</td>\n",
       "      <td>0.008961</td>\n",
       "      <td>0.822344</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>800</td>\n",
       "      <td>400</td>\n",
       "      <td>10</td>\n",
       "      <td>0.004881</td>\n",
       "      <td>0.940452</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>80 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    chunk_size  chunk_overlap  chunks_retieved  precision    recall\n",
       "0          100              0                1   0.188042  0.486034\n",
       "1          100              0                2   0.120373  0.614665\n",
       "2          100              0                3   0.093550  0.702523\n",
       "3          100              0                5   0.073808  0.897149\n",
       "4          100              0               10   0.039737  0.976826\n",
       "..         ...            ...              ...        ...       ...\n",
       "75         800            400                1   0.031116  0.608412\n",
       "76         800            400                2   0.021086  0.778466\n",
       "77         800            400                3   0.014702  0.809186\n",
       "78         800            400                5   0.008961  0.822344\n",
       "79         800            400               10   0.004881  0.940452\n",
       "\n",
       "[80 rows x 5 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"experiment_comparison_table.csv\")\n",
    "df.sort_values(by=[\"chunk_size\", \"chunk_overlap\", \"chunks_retieved\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some plots of this data can be seen by running `plots.py` \n",
    "\n",
    "## 8. Key Insights\n",
    "\n",
    "- **Smaller chunks** offer high precision but may hurt recall (due to missing context).\n",
    "- **Overlap** improves recall by covering context that spans boundaries between chunks.\n",
    "- **Best results** regarding *recall* were observed with:\n",
    "  - **Chunk size**: 200–600 tokens\n",
    "  - **Overlap**: half of the chunk size\n",
    "  - **Number of chunks retrieved**: 10\n",
    "- **Increasing** the number of chunks retrieved always lowered precision but improved recall, which aligns with intuition.\n",
    "\n",
    "---\n",
    "\n",
    "## 9. Conclusion\n",
    "\n",
    "The implemented retrieval system successfully retrieves relevant document excerpts using fixed-size chunking and dense vector representations. Through multiple experiments, the project highlights how chunk size and overlap can significantly affect retrieval quality. The pipeline demonstrates the potential for using simple chunking methods and embeddings for document retrieval tasks.\n",
    "\n",
    "---\n",
    "\n",
    "## 10. Some Thoughts/Observations Regarding the Task\n",
    "\n",
    "Unfortunately, the link provided in the task didn’t lead to an explanation of the **FixedTokenChunker** algorithm, so I had to deduce the chunking process from the code itself.\\\n",
    "While reading the associated paper, I encountered some concerns regarding how the dataset was created. Using a LLM to obtain relevant context for the questions doesn’t guarantee that the retrieved excerpts are always relevant, nor does it ensure that all relevant excerpts are captured. The authors attempted to mitigate the first issue by checking cosine similarity between queries and references, but it still felt somewhat imprecise at times. The second issue—ensuring that all relevant content is retrieved—was not addressed in the paper, and the authors acknowledged this limitation, which I believe is challenging due to the synthetic nature of the dataset. \\\n",
    "Nonetheless, the task was highly interesting, and I gained valuable insights from it.\n",
    "\n",
    "---\n",
    "\n",
    "## 11. Deliverables\n",
    "\n",
    "- `data/questions_state.csv` and `state_of_the_union.md`\n",
    "- `embedding_function.py`\n",
    "- `fixed_token_chunker.py`\n",
    "- `pipeline.ipynb`\n",
    "- `experiment_comparison_table.csv` \n",
    "- `plots.py` (simple script to generate plots from experiment results)\n",
    "- `report.ipynb` (this file)\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
