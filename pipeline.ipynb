{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "from fixed_token_chunker import FixedTokenChunker as FTChunker\n",
    "from embedding_function import embed_texts "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sum_of_ranges(ranges):\n",
    "    if ranges is None:\n",
    "        return 0\n",
    "    return sum(end - start for start,end in ranges)\n",
    "\n",
    "def union(ranges):\n",
    "    if not ranges:\n",
    "        return [] \n",
    "    sorted_ranges = sorted(ranges)\n",
    "    merged_ranges = [sorted_ranges[0]]\n",
    "    for curr in sorted_ranges[1:]:\n",
    "        last_start, last_end = merged_ranges[-1]\n",
    "        if curr[0] <= last_end:\n",
    "            merged_ranges[-1] = (last_start, max(last_end,curr[1]))\n",
    "        else:\n",
    "            merged_ranges.append(curr)\n",
    "    return merged_ranges\n",
    "\n",
    "def intersect_2(range1,range2):\n",
    "    start = max(range1[0],range2[0])\n",
    "    end   = min(range1[1],range2[1])\n",
    "\n",
    "    return (start,end) if end > start else None\n",
    "\n",
    "def get_all_valid(valid,retrieved):\n",
    "    ret = []\n",
    "    for target in valid:\n",
    "        for range in retrieved:\n",
    "            intersection = intersect_2(range,target) \n",
    "            if intersection is not None:\n",
    "                ret.append(intersection)\n",
    "    return union(ret)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "[(1, 7)]\n",
      "None\n",
      "(4, 6)\n",
      "(3, 5)\n",
      "[(1, 2), (3, 5), (6, 7), (4, 5), (2, 5)]\n"
     ]
    }
   ],
   "source": [
    "#tests\n",
    "ranges = [(1,2),(3,7),(4,6),(2,5)]\n",
    "print(sum_of_ranges(ranges))\n",
    "print(union(ranges))\n",
    "union([(1,2)])\n",
    "\n",
    "print(intersect_2(ranges[0],ranges[1]))\n",
    "print(intersect_2(ranges[1],ranges[2]))\n",
    "print(intersect_2(ranges[3],ranges[1]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_calc(retrived, relevant): \n",
    "    numerator_ranges = get_all_valid(retrived, relevant)\n",
    "    numerator = sum_of_ranges(numerator_ranges)\n",
    "    denum_precission = sum_of_ranges(retrived)\n",
    "    denum_recall = sum_of_ranges(relevant)\n",
    "\n",
    "    precission = numerator/denum_precission\n",
    "    recall =  numerator/denum_recall\n",
    "\n",
    "    return precission,recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_golden(references):\n",
    "    ret = []\n",
    "    for reference in references:\n",
    "        ret.append((reference[\"start_index\"], reference[\"end_index\"]))\n",
    "    return ret\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cosine_similarity(query, matrix):\n",
    "    query = query / np.linalg.norm(query)\n",
    "    matrix = matrix / np.linalg.norm(matrix, axis=1, keepdims=True)\n",
    "    return np.dot(matrix, query)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_target_in_document(document, target):\n",
    "    start_index = document.find(target)\n",
    "    if start_index == -1:\n",
    "        return None\n",
    "    end_index = start_index + len(target)\n",
    "    return start_index, end_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_retrieval(dataset, corpus,  embedding_function, corpus_chunks, chunk_embeddings, num_retrieved):\n",
    "    \"\"\"\n",
    "    For each question in the evaluation dataset:\n",
    "      1. Compute query embedding.\n",
    "      2. Retrieve top-N similar corpus chunks.\n",
    "      3. Evaluate retrieval quality.\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    for item in dataset.iloc():\n",
    "        question = item[\"question\"]\n",
    "        references = item['references']\n",
    "        golden_ranges =  get_golden(references)\n",
    "        \n",
    "        # Generate query embedding using the provided embedding function.\n",
    "        query_emb = embedding_function(question)\n",
    "        \n",
    "        # Compute similarity scores between the query and each corpus chunk.\n",
    "        similarities = compute_cosine_similarity(query_emb, chunk_embeddings)\n",
    "        \n",
    "        # Retrieve the indices of the top-N most similar chunks.\n",
    "        top_indices = np.argsort(-similarities)[:num_retrieved]\n",
    "        retrieved_chunks = [corpus_chunks[i] for i in top_indices]\n",
    "        retrieved_ranges = [find_target_in_document (corpus, chunk) for chunk in retrieved_chunks]\n",
    "        \n",
    "\n",
    "        precision,recall = score_calc(retrieved_ranges, golden_ranges)\n",
    "        \n",
    "        results.append({\n",
    "            \"question\": question,\n",
    "            \"retrieved_chunks\": retrieved_chunks,\n",
    "            \"precision\": precision,\n",
    "            \"recall\" : recall\n",
    "        })\n",
    "    \n",
    "    # Compute average metric over all queries.\n",
    "    avg_precission = np.mean([r[\"precision\"] for r in results])\n",
    "    avg_recall = np.mean([r[\"recall\"] for r in results])\n",
    "\n",
    "    return avg_precission, avg_recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_and_embed(chunker, embedding_function):\n",
    "    with open(\"data/state_of_the_union.md\", \"r\", encoding=\"utf-8\") as f:\n",
    "        corpus = f.read()\n",
    "\n",
    "\n",
    "    dataset = pd.read_csv(\"data/questions_state.csv\", encoding = \"utf-8\",  converters={\"references\": json.loads})\n",
    "    \n",
    "    # Process the corpus into chunks using provided chunker.\n",
    "    corpus_chunks = chunker.split_text(corpus)\n",
    "\n",
    "    # print(corpus_chunks[0])\n",
    "\n",
    "    # Generate embeddings for each chunk.\n",
    "    chunk_embeddings = []\n",
    "    for chunk in corpus_chunks:\n",
    "        emb = embedding_function(chunk)\n",
    "        chunk_embeddings.append(emb)\n",
    "\n",
    "    return dataset,corpus,corpus_chunks,chunk_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_pipeline(chunker, embedding_function, num_retrieved):\n",
    "    dataset,corpus,corpus_chunks,chunk_embeddings = chunk_and_embed(chunker,embedding_function)\n",
    "    return evaluate_retrieval(dataset, corpus, embed_texts, corpus_chunks, chunk_embeddings, num_retrieved)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chunker = FTChunker(chunk_size=400,chunk_overlap=200)\n",
    "# run_pipeline(chunker, embed_texts, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n"
     ]
    }
   ],
   "source": [
    "tests = [(100,0), (100,25), (100,50), (200,0), (200,50), (200,100), (400,0), (400, 100), (400,200), (600,0), (600,125) , (600,300), (800,0) , (800,100), (800,200), (800,400)]\n",
    "results = []\n",
    "for c_len,c_overlap in tests:\n",
    "    chunker = FTChunker(chunk_size=c_len,chunk_overlap=c_overlap)\n",
    "    dataset,corpus,corpus_chunks,chunk_embeddings = chunk_and_embed(chunker,embed_texts)\n",
    "    for num in (1,2,3,5,10):\n",
    "        avg_precission,avg_recall = evaluate_retrieval(dataset,corpus,embed_texts,corpus_chunks,chunk_embeddings,num)\n",
    "        results.append({\n",
    "            \"chunk_size\" : c_len,\n",
    "            \"chunk_overlap\": c_overlap,\n",
    "            \"chunks_retieved\": num,\n",
    "            \"precision\": avg_precission,\n",
    "            \"recall\" : avg_recall\n",
    "        })\n",
    "        print(\"ok\")\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(results)\n",
    "df.to_csv(\"experiment_comparison_table.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
